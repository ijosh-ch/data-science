{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification of Iris\n",
    "\n",
    "The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository. It includes three iris species with 50 samples each as well as some properties about each flower. \n",
    "![](L7-img/iris.png)\n",
    "\n",
    "| Dataset             | Iris\n",
    "|---------------------|-----------------------------------|\n",
    "| Number of Instances | 150 (50 per class) |\n",
    "| Number of Attributes | 4 numeric                        |\n",
    "| Attribute            | Sepal length/cm               |\n",
    "| Attribute            | Sepal width/cm                |\n",
    "|  Attribute           | Petal length/cm               |\n",
    "|  Attribute           | Petal width/cm                |\n",
    "| class                | Iris-Setosa                      |\n",
    "| class                | Iris-Versicolour                 |\n",
    "| class                | Iris-Virginica                   |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "column_names = iris.feature_names\n",
    "iris.data[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, 0]  # we only take the first feature\n",
    "y = iris.target\n",
    "X = np.squeeze(X)\n",
    "#plt.figure(figsize=(12,4))\n",
    "ax=sns.swarmplot(x=y,y=X,hue=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:,1]  # we only take the 2nd feature\n",
    "y = iris.target\n",
    "X = np.squeeze(X)\n",
    "#plt.figure(figsize=(12,4))\n",
    "ax=sns.swarmplot(x=y,y=X,hue=y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:,2]  # we only take the 3rd feature\n",
    "y = iris.target\n",
    "X = np.squeeze(X)\n",
    "#plt.figure(figsize=(12,4))\n",
    "ax=sns.swarmplot(x=y,y=X,hue=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "1. What can you say about the two features plotted above with regards to the use of it for classification? (Hint: can you use them for classification?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate this:\n",
    "\n",
    "$P(y|X) = \\frac{P(X|y)P(y)}{P(X)}$\n",
    "\n",
    "Where y is the class variable and X is the feature vector of size N. N tells us how many different types of X attributes there are - eg sepal width, sepal length, etc. \n",
    "\n",
    "So $P(y|X)$ would be the probability of class y given the X attribute (eg sepal length, sepal width). That is if we have an X attribute value , what is its probability for it belonging to class y=0, y=1, y=2. \n",
    "\n",
    "This is exactly what the classifier does, given whatever value of X, find out which class it most probably belongs to. \n",
    "\n",
    "We can find $P(y|X)$ by this formula (assuming that X attributes are all independent): \n",
    "\n",
    "$P(y|x_{1},..,x_{n}) = \\frac{P(x_{1}|y)P(x_{2}|y) ... P(x_{n}|y)P(y)}{P(x_{1})P(x_{2})... P(x_{n})}$\n",
    "\n",
    "Where $x_{1}$ is the sepal width attribute, $x_{2}$ is sepal length attribute, etc.\n",
    "\n",
    "Can be rewritten as: \n",
    "\n",
    "$P(y|x_{1},..,x_{n}) =\\frac{P(y)\\prod_{i=1}^{n}{P(x_{i}|y)}} {P(x_{1})P(x_{2})... P(x_{n})}$\n",
    "\n",
    "$P(y|x_{1},..,x_{n}) \\propto P(y)\\prod_{i=1}^{n}{P(x_{i}|y)}$\n",
    "\n",
    "We just need to calculate $P(x_{i}|y)$\n",
    "\n",
    "We call this approach **naive bayes** because it is naive to assume that the attributes are independent, i.e. the attributes do not influence each other. For example we assume that the petal length and width are not influenced by each other. Nevertheless this assumption works well in many cases. \n",
    "\n",
    "In order to make it easy to calculate $P(x_{i}|y)$ we put the continous data into discrete bins. To calculate the probability we simply count the number of data points in each bins per class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example because X is continous need to discretize it into bins. It makes it easier for us to calculate as we just need to count and divide them. \n",
    "\n",
    "NOTE: This is just a demo, there are better ways to deal with continuous values. \n",
    "\n",
    "So in our example the continous X values are put into 6 separate bins (or categories). You can try to see if more bins are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1495cce107f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#petal length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKBinsDiscretizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ordinal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_binned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'iris' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "X = iris.data[:,2] #petal length\n",
    "X = X.reshape(-1, 1)\n",
    "enc = preprocessing.KBinsDiscretizer(n_bins=6, encode='ordinal').fit(X)\n",
    "X_binned = enc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binned = X_binned.squeeze() # make 1-d array\n",
    "print(X_binned)\n",
    "ax=sns.swarmplot(x=y,y=X_binned,hue=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_binned[y==0])) #count in class zero\n",
    "print(X_binned[y==0])\n",
    "print(X_binned[y==1])\n",
    "print(X_binned[y==2])\n",
    "enc.bin_edges_[0] #bin category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above data, we count how many data points are in X1 bins for each class Y. Fill up the last row.\n",
    "\n",
    "| X1=0  | X1=1  | X1=2 | X1=3  | X1=4 |  X1=5  | Y  |\n",
    "|-----|-----|----|-----|-----|-----|----|\n",
    "| 24  | 26  | 0  |  0  |  0  |  0  | 0  |\n",
    "| 0   | 0   | 25 | 21  |  4  |  0  | 1  |\n",
    "\n",
    "\n",
    "Do the same with another attribute (see below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:,0] #sepal length\n",
    "X = X.reshape(-1, 1)\n",
    "enc = preprocessing.KBinsDiscretizer(n_bins=6, encode='ordinal').fit(X)\n",
    "X_binned = enc.fit_transform(X)\n",
    "X_binned = X_binned.squeeze() # make 1-d array\n",
    "print(X_binned[y==0])\n",
    "print(X_binned[y==1])\n",
    "print(X_binned[y==2])\n",
    "enc.bin_edges_[0] #bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| X2=0  | X2=1  | X2=2 | X2=3  | X2=4 |  X2=5  | Y  |\n",
    "|-----|-----|----|-----|-----|-----|----|\n",
    "| 20  | 20  | 9  |  1  |  0  |  0  | 0  |\n",
    "| 1   | 4   | 16  | 15  |  8  |  6  | 1  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say now i have a new specimen X1=2, X2=3, what is the most probable class?\n",
    "\n",
    "To calculate $P(y|x_{1},..,x_{n}) \\propto P(y)\\prod_{i=1}^{n}{P(x_{i}|y)}$ \n",
    "\n",
    "We just look up the tables we have created:\n",
    "\n",
    "$P(Y=0 | X1=2, X2=3)\\propto (1/3)(0/50)(1/50)$\n",
    "\n",
    "This is the probability of belonging to Class Y=0, given that the data we have is X1=2 and X2=3.\n",
    "\n",
    "One of the values is zero, in case this we use a small value instead of zero so that the zero will not mask out other contribution from different attribute.\n",
    "\n",
    "$P(Y=0 | X1=2, X2=3) \\propto (1/3)(1/50)(1/50) = 0.00013$\n",
    "\n",
    "### Question\n",
    "\n",
    "1. Calculate the rest to find out which class is most probable for new specimen X1=2, X2=3\n",
    "2. Calculate which class is most probable if the sepal length=5.1 and petal length=3.0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given data as below:\n",
    "\n",
    "    @attribute outlook {sunny, overcast, rainy}\n",
    "    @attribute temperature {high, low}\n",
    "    @attribute humidity {high, low}\n",
    "    @attribute windy {TRUE, FALSE}\n",
    "    @class     play {yes, no}\n",
    "\n",
    "    @data\n",
    "    sunny,high,high,FALSE,no\n",
    "    sunny,high,high,TRUE,no\n",
    "    overcast,high,high,FALSE,yes\n",
    "    rainy,low,high,FALSE,yes\n",
    "    rainy,low,high,FALSE,yes\n",
    "    rainy,low,low,TRUE,no\n",
    "    overcast,low,low,TRUE,yes\n",
    "    sunny,high,high,FALSE,no\n",
    "    sunny,low,high,FALSE,yes\n",
    "    rainy,high,high,FALSE,yes\n",
    "    sunny,high,low,TRUE,yes\n",
    "    overcast,high,high,TRUE,yes\n",
    "    overcast,high,high,FALSE,yes\n",
    "    rainy,high,high,TRUE,no\n",
    "\n",
    "\n",
    "### Question \n",
    "1. Should i play if outlook=ovecast, temperate=low, humidity=high, windy=TRUE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
